Week 5:
********************** Boosting *************************
1. Which of the following is NOT an ensemble method?
Ans: single decision trees.

2. Each binary classifier in an ensemble makes predictions on an input xxx as listed in the table below. Based on the ensemble coefficients also listed in the table, what is the final ensemble model's prediction for xxx?
		Classifier coefficient wt 	Prediction for x
Classifier 1		0.61				+1
Classifier 2		0.53				-1
Classifier 3		0.88				-1
Classifier 4		0.34				+1

Ans: -1.

3. (True/False) Boosted trees tend to be more robust to overfitting than decision trees. 
Ans: true.

4. (True/False) AdaBoost focuses on data points it incorrectly predicted by increasing those weights in the data set.
Ans: true.

5. In an iteration in AdaBoost, recall that learning the coefficient wt for learned weak learner ft is calculated by

1/2ln((1−weighted_error(ft​)​)/weighted_error(ft​))

If the weighted error of ft​ is equal to .25, what is the value of wt​? Round your answer to 2 decimal places.
Ans: 0.5 ln((1-0.25)/0.25) = 0.5 * ln 3= 0.5 * 1.1 = .55

6. Which of the following classifiers is most accurate as computed on a weighted dataset? A classifier with:
Ans: 0.1.
7. Imagine we are training a decision stump in an iteration of AdaBoost, and we are at a node. Each data point is (x1, x2, y), where x1,x2 are features, and y is the label. Also included are the weights of the data. The data at this node is:
Weight	x1	x2	y
0.3	0	1	+1
0.35	1	0	-1
0.1	0	1	+1
0.25	1	1	+1

Ans: 
.35/(0.3+0.35+0.1+0.25)=.35/1= 0.35

8. After each iteration of AdaBoost, the weights on the data points are typically normalized to sum to 1. This is used because
Ans: underflow/overflow.

9.Which of the five points (indicated in the second figure) will receive higher weight in the following iteration? Choose all that apply.
Ans: 2,3.

10. Suppose we are running AdaBoost using decision tree stumps. At a particular iteration, the data points have weights according the figure. (Large points indicate heavy weights.)
Ans: 1.

11. (True/False) AdaBoost achieves zero training error after a sufficient number of iterations, as long as we can find weak learners that perform better than random chance at each iteration of AdaBoost (i.e., on weighted data). 
Ans: True.

12. (True/False) For AdaBoost, test error is an appropriate criterion for choosing the optimal number of iterations. 
Ans: False.

12. Let wtbe the coefficient for a weak learner ft. Which of the following conditions must be true so that wt>0 ? 
Ans: ft<0.5

13. (True/False) AdaBoost can boost any kind of classifier, not just a decision tree stump.
Ans: True.
